{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb437c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"OPENPI_DATA_HOME\"] = \"/home/lperez/.cache/openpi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd4a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax\n",
    "import cv2\n",
    "from flax import nnx\n",
    "import time\n",
    "from openpi.models import model as _model\n",
    "import openpi.shared.nnx_utils as nnx_utils\n",
    "import jax.numpy as jnp\n",
    "from openpi.training.config import get_config\n",
    "from openpi.models.tokenizer import PaligemmaTokenizer\n",
    "from openpi.models.model import Observation\n",
    "from openpi.models.pi0 import make_attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58cff26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug line 0-1\n"
     ]
    }
   ],
   "source": [
    "PALIGEMMA_EOS_TOKEN = 1\n",
    "max_decoding_steps = 20\n",
    "temperature = 0.1\n",
    "\n",
    "### Step 1: Initialize model and load pretrained params\n",
    "config = get_config(\"right_pi05_20\")\n",
    "model_rng = jax.random.key(0)\n",
    "rng = jax.random.key(0)\n",
    "model = config.model.create(model_rng)\n",
    "\n",
    "# Load pretrained params\n",
    "print(f\"debug line 0-1\")\n",
    "graphdef, state = nnx.split(model)\n",
    "loader = config.weight_loader\n",
    "params = nnx.state(model)\n",
    "# Convert frozen params to bfloat16.\n",
    "params = nnx_utils.state_map(params, config.freeze_filter, lambda p: p.replace(p.value.astype(jnp.bfloat16)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e7493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_shape = params.to_pure_dict()\n",
    "loaded_params = loader.load(params_shape)\n",
    "state.replace_by_pure_dict(loaded_params)\n",
    "model = nnx.merge(graphdef, state)\n",
    "\n",
    "### Step 2: Construct an observation batch\n",
    "# load 3 images from tmp_test as uint8 format\n",
    "img_share_path = '/home/lperez/main/nh/openpi/test'\n",
    "img_paths = ['nh/1.png', 'nh/2.png', 'nh/3.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f1a5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for img_name in img_paths:\n",
    "    img_path = os.path.join(img_share_path, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img_list.append(img)\n",
    "# Convert images from [0, 255] to [-1, 1] range as expected by the model\n",
    "img_dict = {\n",
    "    \"base_0_rgb\": jnp.array(img_list[0][np.newaxis, :, :, :]).astype(jnp.float32) / 127.5 - 1.0,\n",
    "    \"left_wrist_0_rgb\": jnp.array(img_list[1][np.newaxis, :, :, :]).astype(jnp.float32) / 127.5 - 1.0,\n",
    "    \"right_wrist_0_rgb\": jnp.array(img_list[2][np.newaxis, :, :, :]).astype(jnp.float32) / 127.5 - 1.0,\n",
    "}\n",
    "\n",
    "# Pick up the flascard on the table\n",
    "# Tokenize the prompt\n",
    "high_level_prompt = 'Put the fruits in the basket'\n",
    "low_level_prompt = 'Put the apple in the basket'\n",
    "tokenizer = PaligemmaTokenizer(max_len=50)\n",
    "tokenized_prompt, tokenized_prompt_mask, token_ar_mask, token_loss_mask = tokenizer.tokenize_high_low_prompt(high_level_prompt, low_level_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "61f0b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form a observation\n",
    "data = {\n",
    "    'image': img_dict,\n",
    "    'image_mask': {key: jnp.ones(1, dtype=jnp.bool) for key in img_dict.keys()},\n",
    "    'state': jnp.zeros((1, 32), dtype=jnp.float32),\n",
    "    # 'state': None,\n",
    "    'tokenized_prompt': jnp.stack([tokenized_prompt], axis=0),\n",
    "    'tokenized_prompt_mask': jnp.stack([tokenized_prompt_mask], axis=0),\n",
    "    'token_ar_mask': jnp.stack([token_ar_mask], axis=0),\n",
    "    'token_loss_mask': jnp.stack([token_loss_mask], axis=0),\n",
    "}\n",
    "observation = Observation.from_dict(data)\n",
    "rng = jax.random.key(42)\n",
    "observation = _model.preprocess_observation(rng, observation, train=False, image_keys=list(observation.images.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b41bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  observation = _model.preprocess_observation(None, new_observation, train=False, image_keys=list(observation.images.keys()))\n",
    "observation = jax.tree.map(jax.device_put, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5779788",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.key(42)\n",
    "actions = jnp.zeros((1, 20, 32))\n",
    "real_action_dim = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "87b8bb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task: put the fruits in the basket. Subtask: put the apple in the basket.;\\nAction: '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = observation.tokenized_prompt\n",
    "tokenizer.detokenize(np.array(tok, dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f962c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "        prefix_token_embeddings, prefix_mask, prefix_ar_mask = model.embed_prefix(observation)\n",
    "        prefix_attn_mask = make_attn_mask(prefix_mask, prefix_ar_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "524f1ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True],      dtype=bool)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_ar_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51a1184c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False]],      dtype=bool)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " prefix_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d71a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        ### 1. Subtask-Generation Loss (Cross-Entropy Loss)\n",
    "        # Compute one-hot targets: we predict *next* token, so shift the input tokens by one.\n",
    "        targets = jax.nn.one_hot(\n",
    "            observation.tokenized_prompt[:, 1:],\n",
    "            model.PaliGemma.llm.module.vocab_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d5f5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Use prefix tokens to perform subtask generation (Prefix: images*3, high-level prompt, low-level prompt, state?)\n",
    "        # We input the last token because the last token is used for flow loss\n",
    "        prefix_positions = jnp.cumsum(prefix_mask, axis=1) - 1\n",
    "        (prefix_out, _), kv_cache = model.PaliGemma.llm(\n",
    "            [prefix_token_embeddings, None], \n",
    "            mask=prefix_attn_mask, \n",
    "            positions=prefix_positions, \n",
    "            adarms_cond=[None, None]\n",
    "        )\n",
    "        prefix_out = prefix_out[:, :-1]\n",
    "\n",
    "        # decode from embedding to logits\n",
    "        logits = model.PaliGemma.llm(\n",
    "            prefix_out[:, -targets.shape[1] :], method='deembed'\n",
    "        )\n",
    "        logp = jax.nn.log_softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2477c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Compute CE loss on token targets\n",
    "        assert observation.token_loss_mask is not None, \"Token loss mask is required\"\n",
    "        loss_mask = observation.token_loss_mask[:, 1:]\n",
    "        token_pplx = jnp.sum(targets * logp, axis=-1)\n",
    "        subtask_generation_loss = -jnp.sum(token_pplx * loss_mask, axis=-1) / jnp.clip(jnp.sum(loss_mask, -1), 1)\n",
    "\n",
    "        ### 2. Flow Matching Loss (MSE Loss)\n",
    "        preprocess_rng, noise_rng, time_rng = jax.random.split(rng, 3)\n",
    "        batch_shape = actions.shape[:-2]\n",
    "        noise = jax.random.normal(noise_rng, actions.shape)\n",
    "        time = jax.random.beta(time_rng, 1.5, 1, batch_shape) * 0.999 + 0.001\n",
    "        time_expanded = time[..., None, None]\n",
    "        x_t = time_expanded * noise + (1 - time_expanded) * actions\n",
    "        u_t = noise - actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8592bffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "        suffix_tokens, suffix_mask, suffix_ar_mask, adarms_cond = model.embed_suffix(observation, x_t, time)\n",
    "        input_mask = jnp.concatenate([prefix_mask, suffix_mask], axis=1)\n",
    "        ar_mask = jnp.concatenate([prefix_ar_mask, suffix_ar_mask], axis=0)\n",
    "        attn_mask = make_attn_mask(input_mask, ar_mask)\n",
    "        attn_mask = attn_mask[:, -suffix_tokens.shape[1]:, :] # Q is [B, action_dim, ...], KV is full length\n",
    "        positions = jnp.cumsum(input_mask, axis=1) - 1\n",
    "        positions = positions[:, -suffix_tokens.shape[1]:]\n",
    "        (_, suffix_out), _ = model.PaliGemma.llm(\n",
    "            [None, suffix_tokens], kv_cache=kv_cache, mask=attn_mask, positions=positions, adarms_cond=[None, adarms_cond]\n",
    "        )\n",
    "        v_t = model.action_out_proj(suffix_out[:, -model.action_horizon :])\n",
    "\n",
    "        # Calculate flow loss with true actions (Real Action Dim <= Action Dim (Padding))\n",
    "        flow_loss = jnp.mean(jnp.square(v_t[:, :, :real_action_dim] - u_t[:, :, :real_action_dim]), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c4c68e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = subtask_generation_loss + jnp.mean(flow_loss, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4468155d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([8.817083], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
